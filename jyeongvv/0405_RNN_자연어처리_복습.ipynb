{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTJ7+DlpW88LmbGY7dEPCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyeongvv/DL_study/blob/main/jyeongvv/0405_RNN_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "NeohiL6zijRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 시퀀스 모델\n",
        "* RNN은 동영상, 자연어, 주가 등 **동적**인 데이터를 이용할 때 성능이 극대화됨\n",
        "* **은닉상태(hidden State)**\n",
        "* **재귀적** 구조\n",
        "* 맥락을 인식할 수 있는 신경망\n",
        "* 기울기 소실, 폭발\n",
        "* GRU, 게이트, 업데이트 게이트"
      ],
      "metadata": {
        "id": "atHwx31TpgXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단점\n",
        "> * 단순히 가중치를 반복 사용해 데이터로부터 순서 정보를 추출\n",
        "> * 기울기를 역전파할 때, 1보다 커지면 너무 커지고 1보다 작아지면 0에 수렴\n",
        "> * 시계열 길이가 길어질수록 적용하기 어려움"
      ],
      "metadata": {
        "id": "rV8lojy-SJx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터처리"
      ],
      "metadata": {
        "id": "ZGZ-Z8DBilIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 시간 없애고 날짜만 남기기\n",
        "* 시간을 없애기 위해서 -> datetime -> dt.date \n",
        "* dtype : object -> 문자열(바꿔주기)\n",
        "* between(A, B) A와 B 사이의 날짜만 표시 (포함)\n",
        "* 인덱스를 리셋하고 기존 인덱스를 삭제 -> `reset_index(drop=True)`"
      ],
      "metadata": {
        "id": "sDbZdGlekNA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋"
      ],
      "metadata": {
        "id": "ympu6pj5ksUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `self.data = self.data / np.max(self.data)` -> np.max 해당 배열에서 가장 큰값 -> 0~1 사이로 데이터를 스케일링"
      ],
      "metadata": {
        "id": "wGjfAEZakvLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델"
      ],
      "metadata": {
        "id": "jiwR7O4_lDLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNN 층 정의\n",
        "  * Open, High, Low\n",
        "  * `num_layers` -> RNN층을 몇 개 쌓을까\n",
        "  > * 너무 많으면 -> **기울기 소실** -> 특징을 업데이트 X\n",
        "  > * 너무 적으면 -> **기울기 폭발** -> 학습 X\n",
        "\n",
        "* x -> 층을 거쳐 나온 결과 (출력층), hn은 모든 RNN층의 은닉 상태 (누적 가중치)\n"
      ],
      "metadata": {
        "id": "n_FuuxehlEQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* FC / MLP => 분류 예측\n",
        "  * FC(fully-connected, 전결합층)\n",
        "  * MLP(multi-layer perceptron, 다층 퍼셉트론)층\n",
        "* x = torch.flatten(x) # 예측한 종가를 1차원 표현\n",
        "* out_features -> 회귀문제는 1개 분류는 분류하는 클래스 갯수대로       "
      ],
      "metadata": {
        "id": "jVs51N24mBkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "NYJjy-tfmUaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `optim.zero_grad()` -> 초기 은닉 상태\n",
        "* RNN 레이어 수, 배치 크기, 은닉층의 면(데이터 들어오는 창구) -> `h0 = torch.zeros(5, data.shape[0], 8).to(device)`\n",
        "* 사용가능한 배치 개수를 반환하는 __len__()"
      ],
      "metadata": {
        "id": "fb8NPZbymghS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어처리"
      ],
      "metadata": {
        "id": "Sq389E2Qpan_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 자연어 : 일상적으로 사용하는 말을 프로그래밍 언어와 구분하여 부르는 말\n",
        "* 워드 임베딩 word embedding: 언어의 최소 단위 -> **토큰화** => 벡터화 (라벨 인코딩)\n",
        "자연어(텍스트) 전처리\n",
        "* 형태소분석(기) - Mecab, Khaill, Okt"
      ],
      "metadata": {
        "id": "tlqg-Xp-pmPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어 전처리"
      ],
      "metadata": {
        "id": "p7ONT5WTpcWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 하이퍼 패러미터 정의\n",
        "* 데이터로딩\n",
        "* 훈련셋 / 데이터셋\n",
        "  * label -> 정답\n",
        "---\n",
        "* 단어 사전\n",
        "> * `TEXT.build_vocab(trainset, min_freq=n)` -> n번 이상 등장한 단어로 단어 사전 만들기\n",
        "> * `LABEL.build_vocab(trainset)` -> 학습데이터에서 5번 미만 등장한 데이터는 unk(unknown)으로 대체\n"
      ],
      "metadata": {
        "id": "Mkh3FFc6p8ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN 모델 구현"
      ],
      "metadata": {
        "id": "IlpokCyvsGWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * `n_layers` 층 -> 은닉 벡터들의 층의 갯수 : 엄청 복잡한 모델이 아니라면, n_layers는 2 이하 통상 정의\n",
        "> * `hidden_dim` 층 내부의 너비\n",
        "> * `n_vocab` 단어 임베딩 관련\n",
        "> * `embed_dim` 단어 임베딩 관련\n",
        "> * `n_classes` 최종적으로 분류할 텍스트의 가짓수 (neg, pos)\n",
        "\n",
        "---\n",
        "* 단어 임베딩 -> 단어들을 벡터화(라벨링)\n",
        "* `n_vocab` : 전체 데이터셋의 모든 단어를 사전 형태로 나타냈을 때, 그 사전에 등재된 단어 수\n",
        "* `embed_dim` : 임베딩 -> 단어 텐서가 가지는 차원값\n",
        "* RNN을 통해 생성되는 은닉 벡터의 차원값과 드롭아웃을 정의\n",
        "```\n",
        "self.hidden_dim = hidden_dim\n",
        "self.dropout = nn.Dropout(dropout_p)\n",
        "```\n",
        "\n",
        "* 시계열(순서) 데이터 -> 하나의 텐서 압축 `self.out = nn.Linear(hidden_dim, n_classes)`"
      ],
      "metadata": {
        "id": "B3cLXOGisOVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `y.data.sub_(1)` -> y값에 일괄적으로 1를 빼줘서, 1과 2를 0과 1로 변환\n",
        "* x를 모델에 입력해서 예측값인 logit 계산\n",
        "  * `logit = model(x)` -> logit = 0과 1 사이의 확률\n",
        "  * `loss = F.cross_entropy(logit, y)` -> 예측값과 정답값(라벨) 비교 -> 손실함수\n",
        "---\n",
        "* tensor.max(0) = 각 열에서의 최대값\n",
        "* tensor.max(1) = 각 행에서의 최대값 / [1] -> 최대값의 인덱스 (0,1)\n",
        "* max의 결과값 -> (최대값[0], 그 값의 인덱스(indices)[1])"
      ],
      "metadata": {
        "id": "LMQX8nhetaA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-C0_PwV3t-x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 모델 객체 정의\n",
        "  * 1 : 내부 GRU의 층 개수\n",
        "  * 256 : 모델 내 은닉 벡터의 차원값\n",
        "  * 128 : 임베딩(벡터화)된 토큰의 차원값\n",
        "  * 0.5 : 드롭아웃 비중\n",
        "  > model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)"
      ],
      "metadata": {
        "id": "-zeA88pSt_l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 학습\n",
        "* not best_val_loss = None은 not으로 취급 = 아직 최적 검증 오차가 비어있거나..\n",
        "* val_loss < best_val_loss = 새롭게 구한 검증 오차가 기존에 최적 검증 오차보다 작으면 -> 새로운 모델의 오차가 더 작으면"
      ],
      "metadata": {
        "id": "KjftmLY3uVSB"
      }
    }
  ]
}