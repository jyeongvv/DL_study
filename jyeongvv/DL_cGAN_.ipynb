{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyeongvv/DL_study/blob/main/jyeongvv/DL_cGAN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cGAN으로 원하는 이미지 생성"
      ],
      "metadata": {
        "id": "xK9xZ18zQhZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOBDl2HBQdUa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 패러미터\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 100\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
        "print(f'사용하고 있는 디바이스 : {DEVICE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_21RoYYRo7x",
        "outputId": "5083f89c-c577-4b08-ef9e-d133b1139707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용하고 있는 디바이스 : cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로딩\n",
        "# Fashoin MNIST 데이터셋\n",
        "trainset = datasets.FashionMNIST(\n",
        "    './.data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = trainset,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ")"
      ],
      "metadata": {
        "id": "Ov8dAhjCRxKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36330331-b492-4470-d643-9a566d5200c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8561889.35it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./.data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 147521.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2737369.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5812187.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./.data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성자 (Generator)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(110, 256), # tensor 100개 input -> 110개 -> 100개 + 10개의 라벨을 합쳐줘서 학습\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1024), # 한 층 더 늚 (이전 예제에 비해)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.embed = nn.Embedding(10, 10) # n x 1 -> n x 10 연속적으로 곱하기\n",
        "        # 연속된 임베딩을 층을 만들어줘야 학습에 유리\n",
        "        # 라벨을 받아서 그 라벨값을 임베딩해서 합쳐주는 것\n",
        "    \n",
        "    def forward(self, z, labels): # 가짜 이미지가 될 확률분포 텐서 z\n",
        "        c = self.embed(labels) # 정답값 층을 임베딩한(연속적으로 확장시킨) c\n",
        "        x = torch.cat([z, c], 1) # 라벨과 z를 이어붙임 (무작위 벡터, 클래스 레이블)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "ESZzYytpR0MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cGAN 생성자](https://github.com/BigData23th/Data/raw/main/dl_05_04.png)\n",
        "> cGAN 생성자"
      ],
      "metadata": {
        "id": "3HJdzpVqTxYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 판별자도 레이블 정보를 입력 받음\n",
        "# -> 생성자에서 이미지를 만들 때 쓴 레이블 정보를 입력 받아서\n",
        "# \"레이블이 주어졌을 때 가짜인 확률과 진짜인 확률\"을 추정\n",
        "# 판별자 (Discriminator)\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(10, 10)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28 + 10, 1024), # 794, 1024\n",
        "            # 레이블 정보를 전달하기 위해 이미지 크기 (28*28 = 784)에 10을 더해줌\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3), # 성능 개선용 드롭아웃\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3), # 성능 개선용 드롭아웃\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3), # 성능 개선용 드롭아웃\n",
        "            nn.Linear(256, 1), # 진짜인지 가짜인지 1로 (이진분류)\n",
        "            nn.Sigmoid() # 0~1.\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, labels):\n",
        "        c = self.embed(labels)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        return self.model(x) # 진짜인지 가짜인지 (0, 1)"
      ],
      "metadata": {
        "id": "Q3RyKWC5ULFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cGAN 판별자](https://github.com/BigData23th/Data/raw/main/dl_05_05.png)\n",
        "> cGAN 판별자"
      ],
      "metadata": {
        "id": "8bfrm2BmVuIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 인스턴스 생성\n",
        "D = Discriminator().to(DEVICE)\n",
        "G = Generator().to(DEVICE)\n",
        "\n",
        "# 오차함수 & 최적화함수\n",
        "criterion = nn.BCELoss() # 이진 크로스 엔트로피 (Binary Cross Entropy) 오차함수\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)"
      ],
      "metadata": {
        "id": "ySqGmjF-UP6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(train_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "    # i: 100개인데, 배치 사이즈가 10이면? [0, 1, 2, 3... 9]\n",
        "    # (data, label) -> 이미지, 이미지의 분류\n",
        "    # ---\n",
        "    # for i, (images, _) in enumerate(train_loader): # DataLoader는 BatchSize만큼 끊어서 데이터를 제공\n",
        "    for i, (images, labels) in enumerate(train_loader): # cGAN에서는 라벨이 중요\n",
        "        # BATCH_SIZE, 1, 28, 28 -> BATCH_SIZE, 784\n",
        "        images = images.reshape(BATCH_SIZE, -1).to(DEVICE) # 진짜 이미지\n",
        "\n",
        "        # '진짜'와 '가짜' 레이블 생성\n",
        "        real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE) # 1로 채워진 텐서\n",
        "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE) # 0로 채워진 텐서\n",
        "\n",
        "        # 판별자가 진짜 이미지를 진짜로 인식하는 오차 계산\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = D(images, labels) # 판별자가 진짜 이미지 + 라벨값을 0~1으로 진짜/가짜 여부를 판단\n",
        "        d_loss_real = criterion(outputs, real_labels) # BCELoss\n",
        "        real_score = outputs # 판별자 vs 진짜 이미지\n",
        "\n",
        "        # 무작위 텐서로 가짜 이미지 생성\n",
        "        z = torch.randn(BATCH_SIZE, 100).to(DEVICE) # 정규분포를 따르는 100개의 특성을 가진 가짜 이미지 텐서\n",
        "        g_label = torch.randint(0, 10, (BATCH_SIZE,)).to(DEVICE) # 가짜 이미지의 가짜 답(랜덤으로 만들어진)\n",
        "        # 정규분포로부터 생성된 무작위 텐서를 (생성자 모델이) 입력받아서 실제 이미지와 차원(모양)이 같은 텐서를 생성\n",
        "        fake_images = G(z, g_label)\n",
        "\n",
        "        # 판별자가 가짜 이미지를 가짜로 인식하는 오차를 계산\n",
        "        outputs = D(fake_images, g_label)\n",
        "        # 판별자가 맞추었는가?\n",
        "        d_loss_fake = criterion(outputs, fake_labels) # 오차\n",
        "        fake_score = outputs # 판별자 vs 가짜 이미지\n",
        "\n",
        "        # 진짜와 가짜 이미지를 가지고 낸 오차\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        # 역전파 알고리즘 판별자 모델의 학습을 진행\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step() # 판별자의 패러미터 개선\n",
        "\n",
        "        # 생성자가 판별자를 속였는지에 대해 (생성자 성능) 오차를 계산\n",
        "        fake_images = G(z, g_label)\n",
        "        outputs = D(fake_images, g_label)\n",
        "        # 생성자가 속였는가?\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        # 역전파 알고리즘으로 생성자 모델의 학습을 진행\n",
        "        d_optimizer.zero_grad()\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step() # 생성자의 패러미터 개선\n",
        "    # ---\n",
        "    # 학습 진행도 체크 로그\n",
        "    # 판별자가 진짜를 알아본 정확도 D(x)와 가짜를 진짜로 인식한 정확도 D(G(z))\n",
        "    print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
        "          .format(epoch, EPOCHS, d_loss.item(), g_loss.item(), \n",
        "                  real_score.mean().item(), fake_score.mean().item()))"
      ],
      "metadata": {
        "id": "6GbL0d73ce60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "2251432a-223d-4131-f75e-b43dc85a4c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/300], d_loss: 0.6140, g_loss: 2.4403, D(x): 0.81, D(G(z)): 0.18\n",
            "Epoch [1/300], d_loss: 0.6284, g_loss: 3.0390, D(x): 0.84, D(G(z)): 0.22\n",
            "Epoch [2/300], d_loss: 0.3140, g_loss: 4.3813, D(x): 0.91, D(G(z)): 0.09\n",
            "Epoch [3/300], d_loss: 0.4955, g_loss: 3.3298, D(x): 0.87, D(G(z)): 0.16\n",
            "Epoch [4/300], d_loss: 0.8126, g_loss: 2.3594, D(x): 0.76, D(G(z)): 0.24\n",
            "Epoch [5/300], d_loss: 0.8350, g_loss: 2.1420, D(x): 0.75, D(G(z)): 0.22\n",
            "Epoch [6/300], d_loss: 0.5295, g_loss: 2.0760, D(x): 0.82, D(G(z)): 0.19\n",
            "Epoch [7/300], d_loss: 0.8558, g_loss: 1.7877, D(x): 0.75, D(G(z)): 0.28\n",
            "Epoch [8/300], d_loss: 0.6917, g_loss: 2.1783, D(x): 0.75, D(G(z)): 0.22\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b0600022b2c2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# for i, (images, _) in enumerate(train_loader): # DataLoader는 BatchSize만큼 끊어서 데이터를 제공\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# cGAN에서는 라벨이 중요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# BATCH_SIZE, 1, 28, 28 -> BATCH_SIZE, 784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 진짜 이미지\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}"
      ],
      "metadata": {
        "id": "TFuEL5nUdtDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/BigData23th/Data/raw/main/d2.pt\n",
        "!wget https://github.com/BigData23th/Data/raw/main/g2.pt"
      ],
      "metadata": {
        "id": "oFYU6xWOd54E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37402ae4-34b2-4c58-bb0a-b908f6c562b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 07:31:02--  https://github.com/BigData23th/Data/raw/main/d2.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/BigData23th/Data/main/d2.pt [following]\n",
            "--2023-04-04 07:31:02--  https://raw.githubusercontent.com/BigData23th/Data/main/d2.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5884948 (5.6M) [application/octet-stream]\n",
            "Saving to: ‘d2.pt’\n",
            "\n",
            "\rd2.pt                 0%[                    ]       0  --.-KB/s               \rd2.pt               100%[===================>]   5.61M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-04 07:31:02 (355 MB/s) - ‘d2.pt’ saved [5884948/5884948]\n",
            "\n",
            "--2023-04-04 07:31:02--  https://github.com/BigData23th/Data/raw/main/g2.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/BigData23th/Data/main/g2.pt [following]\n",
            "--2023-04-04 07:31:02--  https://raw.githubusercontent.com/BigData23th/Data/main/g2.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5958676 (5.7M) [application/octet-stream]\n",
            "Saving to: ‘g2.pt’\n",
            "\n",
            "g2.pt               100%[===================>]   5.68M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-04 07:31:02 (332 MB/s) - ‘g2.pt’ saved [5958676/5958676]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D.load_state_dict(torch.load('d2.pt'))\n",
        "G.load_state_dict(torch.load('g2.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8c2b5F5eDXr",
        "outputId": "995abeee-6d64-4ec3-b2c7-4f4310886a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 만들고 싶은 아이템 생성하고 시각화하기\n",
        "# 아이템 번호\n",
        "item_number = 8 #@param {\"type\":\"number\"}\n",
        "z = torch.randn(1, 100).to(DEVICE) # 배치 크기 1\n",
        "g_label = torch.full((1,), item_number, dtype=torch.long).to(DEVICE)\n",
        "sample_images = G(z, g_label) # 텐서\n",
        "# CPU, Numpy -> Matplotlib\n",
        "sample_images_img = np.reshape(sample_images.data.cpu().numpy()\n",
        "                               [0],(28, 28)) # 784\n",
        "plt.title(CLASSES[item_number])\n",
        "plt.imshow(sample_images_img, cmap = 'gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "cellView": "form",
        "id": "NN6IZHU6eIxD",
        "outputId": "7edb5b42-0bd4-4fd4-a1ee-9a796195ffb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}