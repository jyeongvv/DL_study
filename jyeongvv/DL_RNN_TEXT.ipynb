{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyeongvv/DL_study/blob/main/jyeongvv/DL_RNN_TEXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영화 리뷰 감정 분석\n",
        "* RNN은 동영상, 자연어, 주가 등 동적인 데이터를 이용할 때 성능이 극대화\n",
        "* 자연어 : 일상적으로 사용하는 말을 프로그래밍 언어와 구분하여 부르는 말"
      ],
      "metadata": {
        "id": "uUNeCTOxh9xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* IMDB 데이터셋\n",
        "    * 텍스트 형태의 데이터셋\n",
        "    * 50,000건의 영화 리뷰로 이루어져 있음\n",
        "    * 각 리뷰는 다수의 영어 문장들로 이루어져 있으며, 평점이 7점 이상의 긍정적인 영화 리뷰는 2로, 평점이 4점 이하인 부정적인 영화 리뷰는 1로 레이블링"
      ],
      "metadata": {
        "id": "PuJJHJMxiPVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 영화 리뷰 텍스트를 RNN에 입력시켜 영화평의 전체 내용을 압축하고, 이렇게 압축된 리뷰 긍정적인지 부정적인지 판단해주는 간단한 분류 모델을 생성\n",
        "* 한글 데이터도 적용"
      ],
      "metadata": {
        "id": "dFncRVweihga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 워드 임베딩 word embedding: 언어의 최소 단위 -> 토큰화 => 벡터화 (라벨 인코딩)"
      ],
      "metadata": {
        "id": "N9Bpl4PVi0Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자연어 전처리"
      ],
      "metadata": {
        "id": "fLDMNqjHjPVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS0DyICgh4t7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faeb973-9676-44dc-b8f1-1e04e90708d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 1.9.1\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /root/.local/lib/python3.9/site-packages\n",
            "Requires: typing-extensions\n",
            "Required-by: fastai, torchaudio, torchdata, torchtext, torchvision, triton\n",
            "---\n",
            "Name: torchtext\n",
            "Version: 0.10.1\n",
            "Summary: Text utilities and datasets for PyTorch\n",
            "Home-page: https://github.com/pytorch/text\n",
            "Author: PyTorch core devs and James Bradbury\n",
            "Author-email: jekbradbury@gmail.com\n",
            "License: BSD\n",
            "Location: /root/.local/lib/python3.9/site-packages\n",
            "Requires: numpy, requests, torch, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show torch torchtext\n",
        "# torch 1.9.1 / torchtext 0.10.1 (0.13.0 이전 사용)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 특수한 버전으로 만들어주겠다 -> 런타임 다시 시작\n",
        "!pip install torchtext==0.10.1 --user -q"
      ],
      "metadata": {
        "id": "sjL4bssDjxVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 임포트\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data, datasets"
      ],
      "metadata": {
        "id": "R9IOmlG7kGyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 패러미터 정의\n",
        "BATCH_SIZE = 64\n",
        "lr = 0.001\n",
        "EPOCHS = 10\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f'running in {DEVICE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGMPhU6yk2PN",
        "outputId": "bf8a6e3b-415d-48cd-e589-98511b8216cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running in cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩하기\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True) # 데이터를 받아줄 빈 객체\n",
        "# sequential : 순서가 있는 데이터인가? / batch_first : 배치 사이즈 차원을 맨 앞으로 두겠다 / lower : 모두 소문자로 전처리\n",
        "LABEL = data.Field(sequential=False, batch_first=True) # 데이터를 받아줄 빈 객체\n",
        "# 0, 1 긍부정 여부 데이터"
      ],
      "metadata": {
        "id": "_9d_GMs9lkES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련셋, 테스트셋으로 분리\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL) # label -> 정답"
      ],
      "metadata": {
        "id": "JGAiSU5pmiNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"훈련셋 : {len(trainset)}\")\n",
        "print(f\"시험셋 : {len(testset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-jr7qImzcI",
        "outputId": "eadb8a34-945b-446f-c0a3-5f2b5bae0691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련셋 : 25000\n",
            "시험셋 : 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset[0].text[:10] # 자동으로 전처리가 되어 토큰화된 데이터 묶음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBsTyS7-nHKa",
        "outputId": "21e501e9-28bb-40be-99dc-1878915f7504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'documentary',\n",
              " 'presents',\n",
              " 'an',\n",
              " 'original',\n",
              " 'theory',\n",
              " 'about',\n",
              " '\"guns,',\n",
              " 'germs',\n",
              " 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "TEXT.build_vocab(trainset, min_freq=5) # 5번 이상 등장한 단어로 단어 사전 만들기\n",
        "# 학습데이터에서 5번 미만 등장한 데이터는 unk(unknown)으로 대체\n",
        "LABEL.build_vocab(trainset)"
      ],
      "metadata": {
        "id": "HgpjHenFna5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.freqs.most_common(100) # 단어의 등장 빈도"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dZz_lzln2by",
        "outputId": "240d9461-5d48-43ea-ef54-f26828b2d511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 322198),\n",
              " ('a', 159953),\n",
              " ('and', 158572),\n",
              " ('of', 144462),\n",
              " ('to', 133967),\n",
              " ('is', 104171),\n",
              " ('in', 90527),\n",
              " ('i', 70480),\n",
              " ('this', 69714),\n",
              " ('that', 66292),\n",
              " ('it', 65505),\n",
              " ('/><br', 50935),\n",
              " ('was', 47024),\n",
              " ('as', 45102),\n",
              " ('for', 42843),\n",
              " ('with', 42729),\n",
              " ('but', 39764),\n",
              " ('on', 31619),\n",
              " ('movie', 30887),\n",
              " ('his', 29059),\n",
              " ('are', 28743),\n",
              " ('not', 28597),\n",
              " ('film', 27777),\n",
              " ('you', 27564),\n",
              " ('have', 27344),\n",
              " ('he', 26177),\n",
              " ('be', 25691),\n",
              " ('at', 22731),\n",
              " ('one', 22480),\n",
              " ('by', 21976),\n",
              " ('an', 21240),\n",
              " ('they', 20624),\n",
              " ('from', 19934),\n",
              " ('all', 19740),\n",
              " ('who', 19407),\n",
              " ('like', 18779),\n",
              " ('so', 18099),\n",
              " ('just', 17309),\n",
              " ('or', 16769),\n",
              " ('has', 16570),\n",
              " ('her', 16540),\n",
              " ('about', 16486),\n",
              " (\"it's\", 15970),\n",
              " ('some', 15280),\n",
              " ('if', 15189),\n",
              " ('out', 14510),\n",
              " ('what', 14055),\n",
              " ('very', 13633),\n",
              " ('when', 13609),\n",
              " ('more', 13170),\n",
              " ('there', 13094),\n",
              " ('she', 12234),\n",
              " ('would', 12027),\n",
              " ('even', 12010),\n",
              " ('good', 11926),\n",
              " ('my', 11766),\n",
              " ('only', 11566),\n",
              " ('their', 11317),\n",
              " ('no', 11273),\n",
              " ('really', 11065),\n",
              " ('had', 11042),\n",
              " ('which', 10898),\n",
              " ('can', 10797),\n",
              " ('up', 10776),\n",
              " ('were', 10528),\n",
              " ('see', 10410),\n",
              " ('than', 9807),\n",
              " ('we', 9417),\n",
              " ('-', 9355),\n",
              " ('been', 9074),\n",
              " ('into', 8990),\n",
              " ('get', 8959),\n",
              " ('will', 8926),\n",
              " ('story', 8743),\n",
              " ('much', 8739),\n",
              " ('because', 8736),\n",
              " ('most', 8477),\n",
              " ('how', 8456),\n",
              " ('other', 8229),\n",
              " ('also', 8007),\n",
              " ('first', 7985),\n",
              " ('its', 7963),\n",
              " ('time', 7945),\n",
              " ('do', 7904),\n",
              " (\"don't\", 7879),\n",
              " ('me', 7722),\n",
              " ('great', 7714),\n",
              " ('people', 7676),\n",
              " ('could', 7594),\n",
              " ('make', 7590),\n",
              " ('any', 7507),\n",
              " ('/>the', 7409),\n",
              " ('after', 7118),\n",
              " ('made', 7041),\n",
              " ('then', 6945),\n",
              " ('bad', 6816),\n",
              " ('think', 6773),\n",
              " ('being', 6390),\n",
              " ('many', 6388),\n",
              " ('him', 6385)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL.vocab.freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM4KxscLoN68",
        "outputId": "51c67214-849e-4ceb-bae9-4aca75e6dc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'pos': 12500, 'neg': 12500})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용 데이터를 학습셋 80% 검증셋 20%로 나누기\n",
        "trainset, valset = trainset.split(split_ratio=0.8) # 0.8 : 학습셋, 나머지 : 검증셋"
      ],
      "metadata": {
        "id": "2xLLRNJMtl0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 로딩을 위한 데이터 로더\n",
        "# 텍스트 형태의 데이터도 모든 학습 데이터를 한 번에 처리 X\n",
        "# batch(배치) 단위로 쪼개서 학습을 진행 -> 반복할 때마다 배치를 생성해주는 반복자(iterator)\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (trainset, valset, testset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, repeat=True\n",
        ")\n",
        "# 반복자 -> enumerate() 함수에 넣어서 반복해주면(루프) 배치 단위의 데이터셋만 뽑아올 수 있음"
      ],
      "metadata": {
        "id": "ArqdfH65t58I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 속 단어의 개수, 레이블의 수\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "8nAJ4vKIutKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[학습셋] {len(trainset)} / [검증셋] {len(valset)} / 테스트셋 {len(testset)} / [단어수] {vocab_size} / [클래스(라벨)] {n_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtpsx2xmu0HX",
        "outputId": "1cae7525-94c6-4e2c-d9cb-4f6cabb0ba7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[학습셋] 20000 / [검증셋] 5000 / 테스트셋 25000 / [단어수] 46159 / [클래스(라벨)] 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN 모델 구현"
      ],
      "metadata": {
        "id": "VzZwbGL9vQDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_layers, # 층 -> 은닉 벡터들의 층의 갯수 : 엄청 복잡한 모델이 아니라면, n_layers는 2 이하 통상 정의\n",
        "                 hidden_dim, # 층 내부의 너비\n",
        "                 n_vocab, # 단어 임베딩 관련\n",
        "                 embed_dim, # 단어 임베딩 관련\n",
        "                 n_classes, # 최종적으로 분류할 텍스트의 가짓수 (neg, pos)\n",
        "                 dropout_p=0.2\n",
        "                 ):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        # 단어 임베딩 -> 단어들을 벡터화(라벨링)\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        # n_vocab : 전체 데이터셋의 모든 단어를 사전 형태로 나타냈을 때, 그 사전에 등재된 단어 수\n",
        "        # embed_dim : 임베딩 -> 단어 텐서가 가지는 차원값\n",
        "        # RNN을 통해 생성되는 은닉 벡터의 차원값과 드롭아웃을 정의\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # 단어 임베딩을 거친 텐서가 GRU로 입력\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers, batch_first=True)\n",
        "                          \n",
        "        # 시계열(순서) 데이터 -> 하나의 텐서 압축\n",
        "        self.out = nn.Linear(hidden_dim, n_classes)\n",
        "    \n",
        "    def forward(self, x): # 문장들 \n",
        "        x = self.embed(x) # 단어들이 일괄적으로 숫자값을 가지는 텐서로 변환\n",
        "        h_0 = self._init_state(batch_size=x.size(0)) # 젓번째 은닉 벡터 초기값 (H0)\n",
        "        x, _ = self.gru(x, h_0) # 입력 x를 첫번째 은닉 벡터 h_0과 함께 gru에 입력하면\n",
        "        # -> 은닉 벡터들이 시계열 배열 형태로 반환 -> (batch_size, 입력 x의 길이, hidden_dim) 3차원 텐서\n",
        "        # gru를 통해 나온 텐서 x -> [:, -1, :]\n",
        "        # -> 배치 내 모든 시계열 은닉 벡터들의 마지막 토큰들을 내포한 (batch_size, 1, hidden_dim)의 텐서를 추출\n",
        "        # => 텐서 내에 가장 최신 은닉값\n",
        "        h_t = x[:, -1, :] # 모든 데이터를 반영한 핵심값 (압축된 은닉 벡터)\n",
        "        # 특정한 영화평, 영화리뷰에 있는 모든 단어(토큰)들을 압축한 값\n",
        "        h_t = self.dropout(h_t)\n",
        "        logit = self.out(h_t)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size=1):\n",
        "        # parameters() : 해당 신경망 모듈(nn.Module)의 가중치 정보를 iterator 형태로 반환\n",
        "        weight = next(self.parameters()).data # 실제 모델의 가중치\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "        # 기존에 존재하던 가중치와 연결한 뒤 순전파에 사용되는 모양으로 바꾸고, 0으로 초기화"
      ],
      "metadata": {
        "id": "zyzVBCiMvSPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습함수\n",
        "def train(model, optimizer, train_iter):\n",
        "    model.train()\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        # b : 각 행의 인덱스, batch : 미니 배치 (학습을 위해 구분한 배치 사이즈 데이터)\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        # y = pos, neg -> vocab -> 1, 2 -> 0, 1\n",
        "        y.data.sub_(1) # y값에 일괄적으로 1를 빼줘서, 1과 2를 0과 1로 변환\n",
        "        optimizer.zero_grad() # 최적화함수 -> 기울기 계산 리셋\n",
        "        # x를 모델에 입력해서 예측값인 logit 계산\n",
        "        logit = model(x) # logit = 0과 1 사이의 확률\n",
        "        # 손실 함수 -> 손실\n",
        "        loss = F.cross_entropy(logit, y) # 예측값과 정답값(라벨) 비교 -> 손실함수\n",
        "        loss.backward() # 역전파\n",
        "        optimizer.step() # 가중치 갱신"
      ],
      "metadata": {
        "id": "5_4AjwBaM9ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가함수 (성능 측정을 위한)\n",
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for batch in val_iter:\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1)\n",
        "        # 기울기 계산 안함 (optimizer 있을 필요 X)\n",
        "        # x를 모델에 입력해서 예측값인 logit 계산\n",
        "        logit = model(x) # logit = 0과 1 사이의 확률\n",
        "        # 손실 함수 -> 손실\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum') # 오차의 합\n",
        "        total_loss += loss.item()\n",
        "        # 예측값과 정답값이 일치하는 경우\n",
        "        # tensor.max(0) = 각 열에서의 최대값\n",
        "        # tensor.max(1) = 각 행에서의 최대값 / [1] -> 최대값의 인덱스 (0,1)\n",
        "        # max의 결과값 -> (최대값[0], 그 값의 인덱스(indices)[1])\n",
        "        # view -> y와 비교할 수 있게 같은 모양 변형 => 일치하는 건 True(1) 일치하지 않는 건 False(0)\n",
        "        # sum -> 일치하는 것들의 갯수만.\n",
        "        # https://velog.io/@jarvis_geun/torch.argmax-torch.max\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    size = len(val_iter.dataset) # 전체 데이터의 갯수\n",
        "    avg_loss = total_loss / size # 오차 평균\n",
        "    avg_accuracy = corrects / size * 100 # (%) - 정확도 평균\n",
        "    return avg_loss, avg_accuracy"
      ],
      "metadata": {
        "id": "5mNyCizzW7sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 객체 정의\n",
        "# 1 : 내부 GRU의 층 개수\n",
        "# 256 : 모델 내 은닉 벡터의 차원값\n",
        "# 128 : 임베딩(벡터화)된 토큰의 차원값\n",
        "# 0.5 : 드롭아웃 비중\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)"
      ],
      "metadata": {
        "id": "GWpX1Yn4ZBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화함수\n",
        "# 최적화함수를 뭘 쓸지 모르면 Adam 써라!\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "w05XhQQtZhg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "best_val_loss = None # 검증 오차를 최소화\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    # print(\"[에포크: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (epoch, val_loss, val_accuracy))\n",
        "    # https://ooyoung.tistory.com/87\n",
        "    print(f\"[에포크: {epoch}] 검증 오차:{val_loss:5.2f} | 검증 정확도:{val_accuracy:5.2f}\")\n",
        "\n",
        "    # 검증 오차가 가장 적은 최적의 모델\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        # not best_val_loss = None은 not으로 취급 = 아직 최적 검증 오차가 비어있거나\n",
        "        # val_loss < best_val_loss = 새롭게 구한 검증 오차가 기존에 최적 검증 오차보다 작으면\n",
        "        # => 새로운 모델의 오차가 더 작으면\n",
        "        if not os.path.isdir(\"snapshot\"): # 스냅샷 폴더 없으면 만들어주세요\n",
        "            os.makedirs(\"snapshot\") # snapshot 폴더에 저장할 것임\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n",
        "        best_val_loss = val_loss # 오차 갱신"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "iv4lh_IQaHWV",
        "outputId": "695311f9-7944-4398-bf17-053d17361fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-ecde0fdd8327>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# 검증 오차를 최소화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-5f063c63b847>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# 손실 함수 -> 손실\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 예측값과 정답값(라벨) 비교 -> 손실함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 가중치 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "id": "RFuhJ3yXeDUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한글 : 네이버 영화 리뷰 감정 분석"
      ],
      "metadata": {
        "id": "-dURPQNrk9H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://konlpy.org/ko/latest/index.html\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "%cd ../\n",
        "# !curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n",
        "# https://teddylee777.github.io/colab/colab-mecab"
      ],
      "metadata": {
        "id": "YZ7pT26WlEZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://konlpy.org/ko/latest/index.html\n",
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "# https://github.com/e9t/nsmc/\n",
        "data_path = 'https://raw.githubusercontent.com/e9t/nsmc/master/'"
      ],
      "metadata": {
        "id": "FqIjo09xlGTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Mecab() # 형태소 분석기를 통한 토큰화\n",
        "\n",
        "# 훈련 데이터셋 & 시험 데이터셋\n",
        "trainset = pd.read_csv(data_path + \"ratings_train.txt\", sep='\\t') # tab으로 구분되어 있기 때문에 \\t를 구분자로 해서 read\n",
        "testset = pd.read_csv(data_path + \"ratings_test.txt\", sep='\\t')"
      ],
      "metadata": {
        "id": "e_nqPLx3l1u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.head()"
      ],
      "metadata": {
        "id": "OcnUdl3km1nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.drop(columns=['id'], inplace=True)\n",
        "testset.drop(columns=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "S2qSY-Gem7NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.head()"
      ],
      "metadata": {
        "id": "GFZbEGS3nBHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.info()"
      ],
      "metadata": {
        "id": "0f6Nx9CvnXjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 1. 결측치 (nan) 제거\n",
        "# 2. 훈련셋에서 30% -> 검증셋으로 분리\n",
        "train_data = trainset.dropna() # 말뭉치에서 nan 값을 제거\n",
        "test_data = testset.dropna()\n",
        "train_data, valid_data = train_test_split(train_data, test_size=0.3, random_state=71)"
      ],
      "metadata": {
        "id": "YWoqBgR_nLYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT, Label\n",
        "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.morphs,\n",
        "                  lower=False, batch_first=True, fix_length=20) # 모든 품사 (모든 단어)\n",
        "# TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.nouns,\n",
        "#                   lower=False, batch_first=True) # 명사만\n",
        "# LABEL = data.LabelField(batch_first=True, dtype=torch.float)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)"
      ],
      "metadata": {
        "id": "d_qJgkUMntK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치용 (텍스트) 데이터셋으로 변환\n",
        "# input_data = pandas df => iterrows() => row (document, label) => TEXT, LABEL\n",
        "def convert_dataset(input_data, text, label):\n",
        "    list_of_example = [data.Example.fromlist(row.tolist(),\n",
        "                                             fields=[('text', text), ('label', label)])\n",
        "                        for _, row in input_data.iterrows()]\n",
        "    dataset = data.Dataset(examples=list_of_example, fields=[('text', text), ('label', label)])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "gtA7bE93oqMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = convert_dataset(train_data, TEXT, LABEL)\n",
        "valid_data = convert_dataset(valid_data, TEXT, LABEL)\n",
        "test_data = convert_dataset(test_data, TEXT, LABEL)"
      ],
      "metadata": {
        "id": "07KZ2pUHpbiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[0].text, train_data.examples[0].label"
      ],
      "metadata": {
        "id": "pfvtMVE9tJq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[0].text, train_data.examples[0].label"
      ],
      "metadata": {
        "id": "nOaBM6cKpwlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.examples[10].text, train_data.examples[10].label"
      ],
      "metadata": {
        "id": "ZOH3VCnHqD2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전\n",
        "TEXT.build_vocab(train_data, max_size=10000)\n",
        "# TEXT.build_vocab(train_data, min_freq=5)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "metadata": {
        "id": "8HG2-B7VqKo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "metadata": {
        "id": "hgtHACJbqU3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 64\n",
        "# 학습에 사용될 iter 정의\n",
        "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True, repeat=False, sort=False, device=DEVICE\n",
        ")"
      ],
      "metadata": {
        "id": "_BjFEJmAqlvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "metadata": {
        "id": "JwJY5dx8rFim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "9M-Y62norOYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_classes = 2\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
        "# model = BasicGRU(2, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)"
      ],
      "metadata": {
        "id": "8MPMsuSTrRJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "# lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "LIPtY-97rdgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습\n",
        "EPOCHS = 10\n",
        "# EPOCHS = 20\n",
        "best_val_loss = None # 검증 오차를 최소화\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuracy = evaluate(model, val_iter)\n",
        "\n",
        "    print(f\"[에포크: {epoch}] 검증 오차:{val_loss:5.2f} | 검증 정확도:{val_accuracy:5.2f}\")\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        if not os.path.isdir(\"snapshot\"): \n",
        "            os.makedirs(\"snapshot\")\n",
        "        torch.save(model.state_dict(), './snapshot/txtclassification_ko.pt')\n",
        "        best_val_loss = val_loss"
      ],
      "metadata": {
        "id": "c4TwgwP9rlPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('./snapshot/txtclassification_ko.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "id": "AzAMutbAr9zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}