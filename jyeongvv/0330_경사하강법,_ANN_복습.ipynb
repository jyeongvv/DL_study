{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-_UeWRx-lkcy"
      ],
      "authorship_tag": "ABX9TyOtRXnd8+Y7L7jZhSFX6f64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyeongvv/DL_study/blob/main/jyeongvv/0330_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%2C_ANN_%EB%B3%B5%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강법"
      ],
      "metadata": {
        "id": "-_UeWRx-lkcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서"
      ],
      "metadata": {
        "id": "9srKQ-brlnjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치에서 사용하는 가장 기본적인 자료구조 -> 수식 계산\n",
        "* 숫자들을 특정한 모양으로 배열함\n",
        "* 차원 / 랭크\n",
        "  * 랭크0 : 숫자가 하나인 스칼라 `( 1/모양 : []) `\n",
        "    * `tensor.item()` -> 스칼라 값만 뽑아주는 메소드\n",
        "  * 랭크1 : 일렬로 숫자를 나열한 벡터 `( [1, 2, 3] / 모양 : [3] )`\n",
        "  * 랭크2 : 2차원 행렬 Matrix `( [[1, 2, 3]] / 모양 : [1, 3] )`\n",
        "    * [행, 열]\n",
        "  * 랭크3 : 정육면체 같은 3차원 배열 `( [[[1, 2, 3]]] / 모양 : [1, 1, 3] )`\n",
        "    * [깊이, 행, 열]\n",
        "  * 3차원 이상 -> 랭크n"
      ],
      "metadata": {
        "id": "CUiPV4XglpVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.tensor()` -> 텐서를 만들어주는 생성자\n",
        "* 배열 모양 준수! (가로, 세로 모양이 같아야함...) -> 행, 열 갯수 같아야함\n",
        "---\n",
        "* 텐서의 구체적인 형태 = 모든 차원의 원소 개수\n",
        "* `tensor.size()` : 텐서의 크기(모양)\n",
        "* `tensor.shape = tensor.size()`\n",
        "---\n",
        "* 텐서의 랭크/차원\n",
        "* `tenosr.ndimension()` -> 랭크의 차원값\n",
        "* `len(x)` : 원소의 개수\n",
        "---\n",
        "#### unsqueeze\n",
        "* 랭크(차원) 늘리기 -> 랭크/차원이 변화해도 원소 수에는 영향 X\n",
        "* torch.unsqueeze(tensor, n) -> n번째 차원에 1 차원값을 추가함\n",
        "* `try:\n",
        "    add_dimension(3) # 없는 차원에 추가하려고 했을 때\n",
        "except Exception as e:\n",
        "    print(type(e))\n",
        "    print(e)` -> 없는 차원에 추가\n",
        "* 랭크(차원) 줄이기 -> 원소 수 영향 X (모양만 변경)\n",
        "---\n",
        "#### `view` 메소드\n",
        "* tensor.view(모양) : 직접 텐서의 모양 바꾸기\n",
        "*  튜플이나 리스트로 원하는 모양을 넣으면 해당 모양으로 바꿔줌\n",
        "* view를 통해서 모양을 바꿀 때의 패러미터들은 전체 원소 개수의 **약수**들의 조합으로 나타낼 수 밖에 없음 -> 곱하기 한 결과가 전체 원소 수를 넘으면 안됨"
      ],
      "metadata": {
        "id": "43en6XawncNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텐서 연산과 행렬 곱"
      ],
      "metadata": {
        "id": "x90jHJHq5TMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 행렬\n",
        "  * 랭크 2\n",
        "  * 숫자를 네모꼴로 배치한 2차원 배열\n",
        "  * 행 : 행렬의 높이\n",
        "  * 열 : 행렬의 너비\n",
        "  ---\n",
        "#### 곱(multiple)\n",
        "* 두 행렬 A, B의 행렬곱 시 조건\n",
        "  * A의 열 수와 B의 행 수는 같아야함\n",
        "  * 행렬곱 A.B의 결과 행렬의 행 개수는 A와 같고, 열의 개수는 B와 같음 -> 선형대수 행렬 연산과 비슷\n",
        "  * 넘파이와는 다른 연산방식을 사용해야함\n",
        "    * 넘파이는 그냥 곱하기 연산\n",
        "  ---\n",
        "  * 정규분포(normal distribution) : 평균 0이고 표준편차 1인 확률분포\n",
        "  * `torch.randn`(행의 크기, 열의 크기, dtype...)"
      ],
      "metadata": {
        "id": "FoLdKSvP5dZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd"
      ],
      "metadata": {
        "id": "3CHe_wEJAmfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Auto + gradient\n",
        "* **수식의 기울기**를 자동으로 계산\n"
      ],
      "metadata": {
        "id": "_fJ8IQtBB0Kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 머신러닝 모델은 입력된 데이터를 기반으로 학습함\n",
        "* 아직까지는 충분한 데이터를 보지 못해서 학습이 끝나지 않은 모델의 경우는, 정답이 아닌 결과를 출력할 가능성 ↑\n",
        "\n",
        "**거리** `distance`\n",
        "* 데이터에 대한 정답과 머신러닝 모델이 예측한 결과의 차이를 계산, 숫자로 표현한 것\n",
        "* 한개(행)씩의 데이터\n",
        "\n",
        "**오차** `loss`\n",
        "* 학습한 데이터로 계산한 거리들의 평균\n",
        "* 전체 데이터를 기준으로 함\n",
        "* 오차가 작은 머신러닝 모델일 수록 주어진 데이터에 대해 보다 더 정확한 답을 내어줌\n",
        "* 오차를 줄이고 알고리즘 중에 가장 유명하고 많이 쓰이는 알고리즘이 <mark>경사하강법</mark> `gradient descent`\n",
        "\n",
        "**경사하강법**\n",
        "* 오차를 <mark>수학함수로 표현</mark>한 후, 미분하여 함수의 기울기를 구함 -> 오차의 최솟값을 찾아 내는 알고리즘 (예측이 더 정확해진다는 의미)\n",
        "\n",
        "```\n",
        "* 경사하강법 -> 함수가 수정 -> 기울기 -> 오차가 최소화하는 지점으로 변동\n",
        "```\n",
        "\n",
        "> 파이토치의 Autograd는 미분(기울기)계싼을 자동화하여 경사하강법을 구현하는 수고를 덜어준다"
      ],
      "metadata": {
        "id": "bytFuSw_Cjpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `requires_grad=True` : '해당 토치 텐서에 대해 미분값(기울기)을 계산해서 자동으로 .grad 속성에 저장해주세요'라는 뜻\n",
        "\n",
        "### backward `역전파(Backpropagation) 알고리즘`\n",
        "  * 손실 함수를 모델의 파라미터로 미분하는 과정을 의미\n"
      ],
      "metadata": {
        "id": "OZOwFABEF36A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 경사하강법으로 이미지 복원"
      ],
      "metadata": {
        "id": "n2C7DZfwG8T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* weird_function()을 통해 오염된 이미지가 만들어짐\n",
        "* 오염된 이미지와 위 함수를 활용해 원본 이미지 복원이 가능함 "
      ],
      "metadata": {
        "id": "Iit0I0GCG-Ku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제해결"
      ],
      "metadata": {
        "id": "7Z1skUOxHRPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 오염된 이미지와 크기가 같은 랜덤 텐서를 생성\n",
        "* 랜덤텐서를 weird_function()에 입력해 똑같잉 오염된 이미지를 생성해줌 -> 가설\n",
        "  * 원본 이미지 -> weird_function() -> 오염된 이미지\n",
        "  * 랜덤 이미지 -> weird_function() -> 가설\n",
        "* _가설_ = 오염된 이미지라면 무작위 이미지 = 원본 이미지도 성립\n",
        "* `weird_function(랜덤 텐서) = 오염된 이미지` 관계가 성립하도록 만든다"
      ],
      "metadata": {
        "id": "O9dSz2faHTOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "머신러닝 모델이 학습 = 모델이 출력한 결과값과 정답의 차이 -> <mark>오차가 최소화된다</mark>\n",
        "* 오차 : 가설과 원본 이미지가 weird_function() 을 통해 오염된 이미지 사이의 거리"
      ],
      "metadata": {
        "id": "8aixYVpOIHSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**경사하강법 알고리즘**\n",
        "* Autograd 기능을 이용하여 오차를 출력하는 함수의 기울기(미분)에 해당하는 값을 찾을 수 있음\n",
        "> 랜덤 텐서를 미분값의 반대 방향으로 조금씩 이동하면서 모델을 최적화 = 경사하강법 알고리즘"
      ],
      "metadata": {
        "id": "YZJ4ju2pIgfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치를 통해서 처리할 수 있는 데이터 형태 : 텐서\n",
        "\n",
        "```\n",
        "이미지를 오염 시키는 함수\n",
        "def weird_function(x, n_iter=5):\n",
        "    h = x    \n",
        "    filt = torch.tensor([-1./3, 1./3, -1./3])\n",
        "    for i in range(n_iter):\n",
        "        zero_tensor = torch.tensor([1.0*0])\n",
        "        h_l = torch.cat( (zero_tensor, h[:-1]), 0)\n",
        "        h_r = torch.cat((h[1:], zero_tensor), 0 )\n",
        "        h = filt[0] * h + filt[2] * h_l + filt[1] * h_r\n",
        "        if i % 2 == 0:\n",
        "            h = torch.cat( (h[h.shape[0]//2:],h[:h.shape[0]//2]), 0  )\n",
        "    return h\n",
        "```\n",
        "\n",
        "* torch.dist() : 두 텐서 사이의 거리를 구하는 함수"
      ],
      "metadata": {
        "id": "ZV9ms3wgIz0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 학습률 `learning rate(lr)`\n",
        "* 경사하강법은 여러 번 반복해서 이뤄짐\n",
        "*  이때 한 반복에서 최솟점으로 얼마나 많이 이동하는지, 즉 학습을 얼마나 급하게 진행하는지를 정하는 매개변수 = 학습률\n",
        "* 학습률이 너무 크면 오차 함수(손실 함수)의 최솟점을 찾지 못하고 지나치게 되고, 값이 너무 작으면 학습 속도가 느려짐\n",
        "\n",
        "```\n",
        "_경사하강법을 적용하기 위한 반복문 구현_\n",
        "_오차 함수를 random_tensor 미분 -> 기울기 -> 가중치 -> 오차 감소_\n",
        "for i in range(50000):\n",
        "random_tensor.requires_grad_(True)\n",
        "     _random_tensor를 weird_function() 함수에 통과 시킨 후에 hypothesis를 구할 예정_\n",
        "     _가설을 미분 -> random_tensor.grad -> 미분계수_\n",
        "\n",
        "    hypothesis = weird_function(random_tensor)\n",
        "    _distanc_loss() 함수에 hypothesis와 broken_image를 입력해서 오차를 개선_\n",
        "    _random_tensor -> (wf) -> hypothesis -> (dist.loss) -> loss -> 미분해서 연쇄법칙_\n",
        "    \n",
        "    loss = distance_loss(hypothesis, broken_image)\n",
        "    loss.backward()\n",
        "     _loss.backward() 함수를 호출해 loss를 random_tensor 미분_\n",
        "    _자동 기울기 계산을 비활성화_\n",
        "    with torch.no_grad():\n",
        "         _이 안 실행문을 통해서 직접 경사하강법을 구현_\n",
        "        random_tensor = random_tensor - lr * random_tensor.grad\n",
        "        _ random_tensor.grad <- loss.backward() 함수에서 계산한 loss의 기울기_\n",
        "         _loss가 최대값이 되는 곳의 방향_\n",
        "         _-> 이 방향의 반대쪽으로 random_tensor를 lr(학습률)만큼 이동시킴_\n",
        "    if i % 1000 == 0:  _% 1000 으로 나눴을 때의 나머지 0 -> 1001, 2001..._\n",
        "        print(f'Loss at {i} = {loss.item()}')  _item : 텐서의 스칼라값 추출_\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "PXD1OkarMU9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* epoch_test(숫자)로 이미지 확인"
      ],
      "metadata": {
        "id": "1cRBQLHpNaNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 경사하강법으로 사인 함수 예측"
      ],
      "metadata": {
        "id": "2LXmFnoCNkil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**딥러닝 문제 해결 방식**\n",
        "* 모델 정의 -> [모델 순전파(입-출) -> 오차 계산 -> 오차 역전파(가중치 업데이트)] -> 반복(원하는 만큼) -> 오차 최소화 -> 학습종료 및 모델 사용"
      ],
      "metadata": {
        "id": "hJHQTYW_Nnx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* torch.sin(...) -> 해당 벡터에 있는 값들에 대응하는 사인함수 결과값을 반환\n",
        "* torch.randn(()) -> 1개 짜리 랜덤값 (빈 튜플)\n"
      ],
      "metadata": {
        "id": "9p2X0RuwN_aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "QYnk_FwLOa0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN"
      ],
      "metadata": {
        "id": "877i33VlOYE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 신경망 모델 구현하기"
      ],
      "metadata": {
        "id": "5K70r6vkOZMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인공신경망(ANN)"
      ],
      "metadata": {
        "id": "ocgj6Zh3OoYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 입력층 `input` : 자극을 입력받는 감각기관에 해당\n",
        "* 은닉층 `hiddden` : 입력층을 거친 자극이 지나가는 곳\n",
        "* 출력층 `output` : 마지막으로 전달되는 뉴런\n",
        "* 각 층에 존재하는 한 단위의 인공 뉴런 -> 노드 node\n",
        "---\n",
        "* 각 노드는 입력된 자극에 특정한 수학 연산을 실행\n",
        "* 가중치에 행렬곱 시키고 편향을 더해줌\n",
        "  * **가중치(weight)** : 입력 신호가 출력해주는 영향을 계산하는 매개변수\n",
        "  * **편향(bias)** : 노드가 얼마나 데이터에 민감한지 알려주는 매개변수\n",
        "* 결과는 활성화 함수를 거쳐 결과를 산출함\n",
        "  * **활성화함수** : 입력에 적절한 처리를 해서 출력 신호로 변환하는 함수\n",
        "  * 입력신호의 합이 활성화를 일으키는지 여부를 정함 (출력정도도)\n",
        "---\n",
        "* 노드를 통해 나온 결과는 은닉층의 인공뉴런으로 전달되고, 가중치 곱&활성화 함수를 거침\n",
        "* 인공 신경망의 출력층이 낸 결과와 정답을 비교해 오차 계산\n",
        "* 오차를 기반으로 신경망 **전체**를 학습시키려면 모두 (입출력 가중치) 경사하강법을 활용해 변경해줘야함\n",
        "* 겹겹이 쌓인 가중치를 뒤에서부터 차례대로 조정하고 최적화하는 알고리즘 = 역전파 알고리즘(오차역전파)"
      ],
      "metadata": {
        "id": "mjQy7OcEOtDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 간단한 분류 모델 구현"
      ],
      "metadata": {
        "id": "CpiXe-ywQW94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* make_blobs() : 데이터를 2차원 벡터 형태로 만든 데이터셋을 생성"
      ],
      "metadata": {
        "id": "R6mZ_3t3Qc0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`__init__`**\n",
        "\n",
        "\n",
        "```\n",
        "신경망 모델을 구현\n",
        "파이토치에서는 신경망을 클래스 구현\n",
        "파이토치 -> 모듈. 이미 기본적인 설계도를 구현\n",
        "torch.nn.Module -> 상속\n",
        "class 이름: + 클래스에 들어갈 기능(self, ...):\n",
        "```\n",
        "**예시**\n",
        "```\n",
        "class Glory:\n",
        "  def __init__(self): # 자동호출\n",
        "    self.name = '연진'\n",
        "  def say(self):\n",
        "    print(f\"{self.name}아 나 지금 신나\")\n",
        "\n",
        "a = Glory()\n",
        "a.say()\n",
        "a.name\n",
        "```\n",
        "**예시2**\n",
        "```\n",
        "상속하고 싶은 클래스를 옆에 괄호에 넣어줌\n",
        "class GloryPt3(GloryPt2):\n",
        "  pass\n",
        "\n",
        "b = GloryPt2('혜정')\n",
        "b.say()\n",
        "b.name\n",
        "```"
      ],
      "metadata": {
        "id": "kakLQio7QjEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 신경망 모델 구현"
      ],
      "metadata": {
        "id": "EOJBVjSURKUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* torch.nn.Module -> 상속 : 이미 구현된 클래스의 기능과 속성을 사용하는 것\n",
        "* class NeuralNet(torch.nn.Module):\n",
        "  * torch.nn.Module : 부모 클래스 / NeuralNet : 자식 클래스\n",
        "\n",
        "  ```\n",
        "    # 생성자 __init__\n",
        "    # 신경망의 구조와 동작을 (부여, 결정) 정의하는 생성자를 모델 클래스(NeuralNet)에 정의(기록)\n",
        "    # __init__() : 파이썬에서 객체가 갖는 속성값을 초기화하는 역할\n",
        "    # 초기화 (initialization) : 할당된 자리에 값을 채워줌 / 어떤 값을 대입해 줌\n",
        "    # 정의 (definition) : (변수등의) 자리를 만들어 줌\n",
        "    # __init__ -> 객체가 생성되면 자동으로 호출(실행)됨\n",
        "    def __init__(self, input_size, hidden_size): # 입력층과 은닉층의 사이즈를 초기화 시 결정\n",
        "        # 이미 정의된 nn.Module 활용해서 모델을 구성\n",
        "        # super.__init__() # 생성자를 super를 통해 직접 실행해서 부모 클래스의 생성자를 작동시킴\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # super() 함수를 부르면 nn.Module(부모 클래스)의 속성들을 가지고 초기화\n",
        "        # ---\n",
        "        # 객체를 만들 때 input_size, hidden_size 입력\n",
        "        # input_size : 입력층 차원, hidden_size : 은닉층 차원\n",
        "        # self는 생성될 객체 그 자체를 의미\n",
        "        self.input_size = input_size\n",
        "        # self -> input_size 속성이 생성 -> __init__ 전달받은 input_size가 부여\n",
        "        self.hidden_size = hidden_size\n",
        "        # self -> hidden_size 속성이 생성 -> __init__ 전달받은 hidden_size가 부여\n",
        "\n",
        "        # 인공 신경망 연산 정의 (층을 이동할 때 어떠한 계산이 일어날지)\n",
        "        # nn.Linear - 선형 결합, 행렬곱(가중치)과 편향(bias)를 포함하는 연산\n",
        "        # 입력층 -> 입력을 받아서 은닉층으로 넘겨줄 것\n",
        "        self.linear_1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        # 들어올 크기 -> 나갈 크기\n",
        "        self.relu = torch.nn.ReLU() # 활성화 함수\n",
        "        # linear_1 층을 통해서 -> 가중치 계산 -> relu 필터링 (0보다 작으면 0, 그보다 크면 그대로)\n",
        "        # 은닉층 -> 출력\n",
        "        self.linear_2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "        # 이진분류 -> linear_1의 결과물을 받아서 -> 1개의 출력을 몰아줌 (0, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid() # 활성화 함수\n",
        "        # linear_1, relu, ... -> 함수처럼 쓰임\n",
        "    \n",
        "    # 가중치를 통해서 입력받은 값들을 변환하는 작업 : 순전파\n",
        "    def forward(self, input_tensor): # 학습에 쓰일 텐서 (데이터)\n",
        "        # init() 함수에서 정의된 동작들을 차례로 실행\n",
        "        # linear1 : 입력 데이터 (input_tensor)에 [input_size, hidden_size] 크기의 가중치를 행렬 곱하고\n",
        "        # 편향을 더해서 [1, hidden_size] 의 텐서를 반환\n",
        "        linear1 = self.linear_1(input_tensor)\n",
        "        relu = self.relu(linear1) # [1, hidden_size]\n",
        "        # linear2 : [1, 1] 모양으로 변환\n",
        "        linear2 = self.linear_2(relu)\n",
        "        output = self.sigmoid(linear2)\n",
        "        return output\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        " #### **ReLU**\n",
        "  * ReLU : 입력값이 0보다 작으면 0을, 0보다 크면 입력값을 그대로 출력함\n",
        "\n",
        "#### **시그모이드 sigmoid**\n",
        "* 0과 1 사이의 값을 반환\n",
        "* 데이터를 0과 1 사이의 임의의 수로 제한해주어 어디에 더 가까운지 알 수 있음"
      ],
      "metadata": {
        "id": "wFoppEGZRL6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델링 진행"
      ],
      "metadata": {
        "id": "3ZyKeI-4RyHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 신경망 객체를 생성\n",
        "  * odel = NeuralNet(n, m)  _입력층 레이어 n, 은닉층 레이어 m_\n",
        "* 학습 관련 변수와 알고리즘 정의\n",
        "  * 학습률 설정 -> `learning _rate`\n",
        "* 에포크(epoch) : 전체 학습 데이터를 총 몇 번 모델에 입력할지 결정\n",
        "  * 너무 작게하면 모델의 학습이 충분하지 않음 -> 언더피팅, 과소적합\n",
        "  * 너무 크게하면 학습이 오래걸리고 새로운 데이터에 적응못함 -> 오버피팅, 과적합\n",
        "* 학습에 사용할 최적화 알고리즘 결정\n",
        "  * optimizer.step() 함수를 호출할 때마다 가중치를 학습률만큼 갱신\n",
        "  * model.parameters() 함수로 모델 내부의 가중치를 추출 -> 가중치, 학습률\n",
        "* 모델의 성능 시험(학습 X)\n",
        "  * `model.eval()` : evaluate (nn.Module) -> 시험모드로 설정\n",
        "  * 모델의 결과값과 레이블값(정답값)의 차원을 맞추기 위해 squeeze() -> 오차 구하기"
      ],
      "metadata": {
        "id": "UQyhgfV5Rzdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습을 통한 성능 개선"
      ],
      "metadata": {
        "id": "BnxvvajES8uY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "for epoch in range(epochs): # 2000번 돌리기\n",
        "    # 학습 모드로 전환\n",
        "    model.train()\n",
        "    \n",
        "    # 한 번 반복해주는 에포크마다, 새로운 경사값을 계산할 것이므로\n",
        "    # zero_grad() 함수를 호출해서 경사값(기울기)을 0으로 설정\n",
        "    optimizer.zero_grad() # 그 전 기울기가 저장이 되어 있으므로 reset\n",
        "    # 학습 데이터를 입력해서 결과값을 개선\n",
        "    train_output = model(x_train) # x_train을 넣었을 때의 결과물\n",
        "    # nn.module -> 알아서 forward()를 호출해줌\n",
        "    \n",
        "    # 결과값의 차원과 레이블의 차원을 같게 만들고 오차를 계산\n",
        "    # loss_function\n",
        "    train_loss = criterion(train_output.squeeze(), y_train) # x_train -> 모델을 통해 나온 예측값 vs 실제 정답값 비교\n",
        "\n",
        "    # 100 에포크마다 오차를 출력해서 학습이 잘 되는지 확인 \n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"train loss at {epoch} : {train_loss.item()}\")\n",
        "    \n",
        "    # 오차함수(손실함수)를 가중치로 미분하여 오차가 최소가 되는 방향을 구하고,\n",
        "    # 그 방향으로 모델을 학습률만큼 이동시킴 (오차 역전파)\n",
        "    train_loss.backward()\n",
        "    optimizer.step() # optimizer.step() 함수를 호출할 때마다 가중치를 학습률만큼 갱신\n",
        " ```"
      ],
      "metadata": {
        "id": "PnW0nZhOS-qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 후 성능 척정"
      ],
      "metadata": {
        "id": "oyTDuqALTGTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 평가 모드로 바꾸기 + 테스트 데이터인 x_test & y_test를 통해 오차 구해보기\n",
        "* 모델을 저장 -> 딥러닝에서 모델을 저장 -> 레이어마다의 가중치를 저장\n",
        "* model.state_dict() : 모델 내의 가중치들을 딕셔너리 형태\n",
        "* {연산 이름: 가중치 텐서와 편향 텐서}와 같이 표현된 데이터\n",
        "  * `torch.save(model.state_dict(), './model.pt')`\n",
        "* 사용하려고 하는 모델 자체는 직접 구현 (Class 선언)\n",
        "* .load_state_dict -> 이미 학습된 모델의 가중치를 적용"
      ],
      "metadata": {
        "id": "5dLh2uSQTKfA"
      }
    }
  ]
}